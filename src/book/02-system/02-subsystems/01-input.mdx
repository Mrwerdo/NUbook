---
section: System
chapter: Subsystems
title: Input
description: How inputs are handled in the NUbots codebase.
slug: /system/subsystems/input
---

Input to the system includes cameras, [Game Controller](https://github.com/RoboCup-Humanoid-TC/GameController) and [NatNet](https://optitrack.com/products/natnet-sdk/).

## Cameras

The cameras have the following parameters that are used by the object detection algorithm.

| Parameter           | Type                 | Description                                                                                                                                           |
| ------------------- | -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `serial_number`     | string               | The serial number of the camera. This is used to identify the camera and distinguish it from other cameras in the robot.                              |
| `lens.projection`   | string               | The lens projection type. Can be rectilinear, equidistant or equisold.                                                                                |
| `lens.focal_length` | float                | The normalised focal length. It is defined as focal length in pixels divided by image width. The focal length is the angle of view and magnification. |
| `lens.center`       | 2-dimensional vector | The normalised image centre offset. Represents the pixels from the centre of the image to the optical axis, divided by the image width                |
| `k`                 | 2-dimensional vector | The polynomial distortion coefficients for the length                                                                                                 |
| `fov`               | float (radians)      | Field of view. The angular diameter that the lens covers (the area that light hits on the sensor).                                                    |
| `Hpc`               | 4x4 matrix           | Homogeneous transform from the rigid platform this camera is attached to (pitch servo) to the camera's virtual focal point.                           |

These parameters are set for each camera as **configuration values** in the [Camera module](https://github.com/NUbots/NUbots/tree/main/module/input/Camera/data/config), in each robot's respective folder. The values for the left camera on the robot will be stored in `Left.yaml`. The values for the right camera on the robot will be stored in `Right.yaml`.

The parameters are used in the **Camera module** to find and set up the cameras. The Camera module emits [Image messages](https://github.com/NUbots/NUbots/tree/main/shared/message/input/Image.proto).

The [projection tool](https://github.com/NUbots/NUbots/tree/main/shared/utility/vision/projection.h), based on [panotools' fisheye projection calculations](https://wiki.panotools.org/Fisheye_Projection), maps a portion of the surface of a sphere to a flat image. The type of projection is specified by the above parameter `lens.projection`.

## Speech and Intention Recognition

The NUbots codebase includes intention recognition functionality, which is used for the RoboCup@Home league.
Speech recognition is the task of converting a spoken sentence into a string of text.
Intention is the task of converting a string of text into a meaningful format for a computer.
[voice2json](https://voice2json.org) is used for both the speech and recognition tasks. The system
is offline and does not use any cloud resources to perform speech or intention recognition.

The way the system works can be broken down as follows:

1. `/home/nubots/.local/share/...` specifies the path to the voice2json [profile](https://voice2json.org/profiles.html).
   The important files are `sentences.ini` and the files located in `slots`.
2. voice2json program provides two commands `transcribe-stream` and `recognize-intent`.
3. The [SpeechIntent](https://github.com/NUbots/module/behaviour/input/SpeechIntent/readme.md) module starts two subprocesses,
   `transcribe-stream` and `recognize-intent`, and emits a NUclear message representing the recognized intention.
4. `transcribe-stream` calls `arecord` under the hood to capture audio from the microphone.
   This does not work in a virtualised environment, therefore `transcribe-wav` is used for testing wav file recordings.

The output of `transcribe-stream` is a json object containing the recognized text, among other things.
The output of `recognize-intent` is a json object containing both the recognized text and the recognized intention.

### Usage

See the SpeechIntent module for details on how to respond to speech recognition events and run the
voice2json commands manually.

The sentences accepted by vocie2json are located in `docker/voice2json/sentences.ini` and are
copied into `/home/nubots/.local/share/voice2json/en-us_kaldi-zamia/sentences.ini` during the build.
The template language used for the file is [described here](http://voice2json.org/sentences.html).
Files located in `slots/*` are referenced inside of the `sentences.ini`.
For example, if the type of objects in a room changes you can modify `slots/objects` and retrain to accept the new objects.

A typical workflow for iterating on the sentences.ini file is to:

1. Somehow edit the file located at `/home/nubots/.local/share/voice2json/en-us_kaldi-zamia/sentences.ini`
2. Run `voice2json train-profile --profile en`
3. Run the speech and intention recognition commands.

<Alert type='info'>
`\` seems to be a line continuation, but this isn't documented.
If you have hard to read expressions, you can try breaking them up over new lines the same as in a shell.
</Alert>

### Dependencies

Under the hood, voice2json has a few major dependencies:

1. [OpenFST](https://www.openfst.org)
2. [Kaldi](https://kaldi-asr.org)
3. [NGram](https://www.opengrm.org/twiki/bin/view/GRM/NGramLibrary)
4. [Phonetisaurus](https://github.com/AdolfVonKleist/Phonetisaurus)

Kaldi is the speech recognition module that has been installed. Voice2json can work with different
speech recognition modules such as:
[PocketSphinx](https://github.com/cmusphinx/pocketsphinx),
[DeepSpeech](https://github.com/mozilla/DeepSpeech),
or [Julius](https://github.com/julius-speech/julius).

The PKGBUILD files for OpenFST, Kaldi, NGram, and Phonetisaurus have been copied from their AUR
packages to prevent any changes from breaking the docker build. There were also additional
modifications that needed to be made to these scripts in order to get them to build correctly.

Kaldi in particular has no semantic versioning and is actively maintained on the `master` branch.
The build script for Kaldi checks out a particular version to prevent the builds from randomly breaking.

### Future Directions

1. Voice2json supports wake words using [Precise](https://github.com/MycroftAI/mycroft-precise).
   This could be used instead of putting the robot into always listening modes.
2. Voice2json supports multiple languages. You could try adding multi-lingual support.
